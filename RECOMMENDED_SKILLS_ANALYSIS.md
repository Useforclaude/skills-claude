# üéØ Skills ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å Projects ‡∏ó‡∏µ‡πà‡∏°‡∏µ

**‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà:** 2025-10-24
**‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å:** Projects ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô `/home/u-and-an/projects/`

---

## üìä ‡∏™‡∏£‡∏∏‡∏õ Projects ‡∏ó‡∏µ‡πà‡∏°‡∏µ

‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏ß‡∏à projects ‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ:

### ‚úÖ Active Projects (3)
1. **video-translation-platform** - Platform ‡∏£‡∏ß‡∏° transcription + translation + voice synthesis
2. **quantum-sync-v5** - SRT ‚Üí Audio synthesis (ultra-accurate timeline sync)
3. **video-translater** - Thai video ‚Üí Thai/English SRT transcription

### üîç Archive Projects (5)
4. ebook-making - Ebook creation tools
5. ebook-book2 - Specific ebook project
6. number-ML - ML for number prediction
7. number_auction - Auction system
8. tiktok-automation - TikTok content automation

### üéØ Domain ‡∏ó‡∏µ‡πà‡∏û‡∏ö
- **Audio/Video Processing** (Whisper, TTS, FFmpeg)
- **Translation** (Thai ‚Üî English)
- **Machine Learning** (voice DNA, number prediction)
- **Automation** (TikTok, content generation)
- **Document Processing** (Ebook creation)

---

## üéØ Skills ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° Priority)

---

## ‚≠ê Priority 1: Audio/Video Processing Skills (‡∏Ñ‡∏∏‡πâ‡∏°‡∏°‡∏≤‡∏Å!)

### 1. **whisper-transcription-skill** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**‡∏ó‡∏≥‡πÑ‡∏°‡∏Ñ‡∏∏‡πâ‡∏°:**
- ‚úÖ ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô 2-3 projects
- ‚úÖ Whisper ‡∏°‡∏µ quirks/best practices ‡πÄ‡∏¢‡∏≠‡∏∞
- ‚úÖ Claude ‡∏£‡∏π‡πâ general ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ production tips

```yaml
---
name: whisper-transcription-skill
description: Expert guide for Whisper audio transcription (OpenAI Whisper large-v3). Use for: Thai transcription, multilingual transcription, accuracy optimization, prompt engineering, VAD integration, chunk strategies, GPU optimization, and production best practices.
---

# Whisper Transcription Expert Skill

## Core Competencies

### 1. Model Selection

**Whisper Models (Accuracy vs Speed):**
```
Model          Size    Accuracy    Speed    VRAM    Use Case
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
tiny           39M     Poor        Fast     1GB     Testing only
base           74M     Fair        Fast     1GB     Quick drafts
small          244M    Good        Medium   2GB     General use
medium         769M    Better      Slow     5GB     Quality work
large-v2       1.5GB   Best        Slow     10GB    Production
large-v3       1.5GB   Best+       Slow     10GB    Production (Latest)
```

**For Thai Language:**
- ‚úÖ **large-v3** (Best accuracy: 95%+)
- ‚ö†Ô∏è medium (Acceptable: 85-90%)
- ‚ùå small/base (Poor: <80%)

### 2. Thai Language Optimization

**Initial Prompt (Critical for Thai!):**
```python
# ‚úÖ CORRECT - Dramatically improves accuracy
whisper_result = model.transcribe(
    audio,
    language="th",  # Explicitly set Thai
    initial_prompt="‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Forex, ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î, ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô"  # Context
)

# ‚ùå WRONG - Auto-detect often fails
whisper_result = model.transcribe(audio)
```

**Why Initial Prompt Matters:**
- Provides context (Forex, trading terms)
- Primes the model for domain vocabulary
- Reduces hallucinations
- **Improves accuracy by 5-10%**

**Domain-Specific Prompts:**

```python
# Financial/Forex content
initial_prompt = """
‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Forex ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î ‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô
‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå: ‡∏£‡∏≤‡∏Ñ‡∏≤, ‡∏Å‡∏£‡∏≤‡∏ü, ‡πÅ‡∏ô‡∏ß‡∏ï‡πâ‡∏≤‡∏ô, ‡πÅ‡∏ô‡∏ß‡∏£‡∏±‡∏ö, ‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πå, ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì
"""

# Technical content
initial_prompt = """
‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏°‡∏¥‡πà‡∏á ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå
"""

# General content
initial_prompt = """
‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
"""
```

### 3. Chunk Strategy (For Long Videos)

**Problem:** Whisper has max length (~30 seconds optimal)

**Solution: Smart Chunking**

```python
from pydub import AudioSegment
from pydub.silence import detect_nonsilent

def smart_chunk_audio(audio_path, max_duration_ms=30000):
    """
    Split audio at silence points (not mid-word!)
    """
    audio = AudioSegment.from_file(audio_path)

    # Detect voice activity
    nonsilent_ranges = detect_nonsilent(
        audio,
        min_silence_len=500,    # 500ms silence = break point
        silence_thresh=-40      # dB threshold
    )

    chunks = []
    current_chunk_start = 0
    current_duration = 0

    for start, end in nonsilent_ranges:
        segment_duration = end - start

        if current_duration + segment_duration > max_duration_ms:
            # Save current chunk
            chunks.append((current_chunk_start, start))
            current_chunk_start = start
            current_duration = 0

        current_duration += segment_duration

    # Last chunk
    if current_duration > 0:
        chunks.append((current_chunk_start, len(audio)))

    return chunks
```

**Why This Works:**
- ‚úÖ Splits at natural pauses
- ‚úÖ Never cuts mid-word
- ‚úÖ Maintains context
- ‚úÖ Better accuracy than fixed-length chunks

### 4. VAD (Voice Activity Detection)

**Use VAD to skip silence:**

```python
import webrtcvad
import wave

def remove_silence_vad(audio_path, output_path):
    """
    Remove silence using WebRTC VAD
    Speeds up transcription 2-3x!
    """
    vad = webrtcvad.Vad(2)  # Aggressiveness: 0-3

    audio = AudioSegment.from_file(audio_path)
    audio = audio.set_frame_rate(16000)  # VAD requires 16kHz

    # Process in 30ms frames
    frame_duration = 30  # ms
    frames = []

    for i in range(0, len(audio), frame_duration):
        frame = audio[i:i+frame_duration]

        if vad.is_speech(frame.raw_data, 16000):
            frames.append(frame)

    # Combine voice-only frames
    result = sum(frames)
    result.export(output_path, format="wav")
```

**Benefits:**
- ‚ö° 2-3x faster (skip silence)
- üí∞ Save GPU credits
- üìä Better SRT timing

### 5. Post-Processing (Critical!)

**Clean Whisper Output:**

```python
def clean_whisper_transcript(text):
    """
    Remove Whisper hallucinations and artifacts
    """
    import re

    # Remove common hallucinations
    hallucinations = [
        r'\[.*?\]',              # [Music], [Applause]
        r'\(.*?\)',              # (laughs), (coughs)
        r'‚ô™.*?‚ô™',                # ‚ô™ music notes ‚ô™
        r'www\..*?\.com',        # URLs
        r'‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏£‡∏±‡∏ö' * 5,       # Repeated thanks (hallucination)
    ]

    for pattern in hallucinations:
        text = re.sub(pattern, '', text)

    # Fix spacing
    text = re.sub(r'\s+', ' ', text)  # Multiple spaces ‚Üí single
    text = text.strip()

    # Fix Thai punctuation
    text = re.sub(r'\s+([.,!?])', r'\1', text)  # Remove space before punctuation

    return text
```

### 6. GPU Optimization

**Batch Processing:**

```python
import torch

# Check GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load model once
model = whisper.load_model("large-v3", device=device)

# Process multiple files
for audio_file in audio_files:
    # Enable FP16 for 2x speedup
    result = model.transcribe(
        audio_file,
        language="th",
        fp16=True,  # ‚Üê 2x faster on GPU!
        beam_size=5,
        best_of=5,
        temperature=0.0  # Deterministic output
    )
```

**GPU Memory Management:**

```python
import gc

def transcribe_with_cleanup(audio_path):
    result = model.transcribe(audio_path)

    # Free GPU memory
    torch.cuda.empty_cache()
    gc.collect()

    return result
```

### 7. SRT Generation

**Convert Whisper Output ‚Üí SRT:**

```python
def whisper_to_srt(segments, output_path):
    """
    Generate SRT file from Whisper segments
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        for i, segment in enumerate(segments, start=1):
            # SRT index
            f.write(f"{i}\n")

            # Timestamp
            start = format_timestamp(segment['start'])
            end = format_timestamp(segment['end'])
            f.write(f"{start} --> {end}\n")

            # Text
            text = clean_whisper_transcript(segment['text'])
            f.write(f"{text}\n\n")

def format_timestamp(seconds):
    """
    Convert seconds ‚Üí SRT timestamp (HH:MM:SS,mmm)
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)

    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"
```

### 8. Common Pitfalls & Solutions

**Problem 1: Hallucinations**

```python
# ‚ùå WRONG - Whisper hallucinates when input is silence
result = model.transcribe("silence.mp3")
# Output: "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏£‡∏±‡∏ö..." (repeated)

# ‚úÖ CORRECT - Use VAD to skip silence
audio = remove_silence_vad("input.mp3", "voice_only.mp3")
result = model.transcribe("voice_only.mp3")
```

**Problem 2: Language Switching**

```python
# ‚ùå WRONG - Whisper switches to English mid-sentence
result = model.transcribe(audio)
# Output: "‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ trading strategy ‡∏ó‡∏µ‡πà‡∏î‡∏µ" ‚Üí "This is a trading strategy..."

# ‚úÖ CORRECT - Force language
result = model.transcribe(
    audio,
    language="th",
    condition_on_previous_text=True  # Use context from previous chunks
)
```

**Problem 3: Poor Timing**

```python
# ‚ùå WRONG - Default word timestamps
result = model.transcribe(audio, word_timestamps=False)
# Timing: Inaccurate segments

# ‚úÖ CORRECT - Enable word-level timestamps
result = model.transcribe(
    audio,
    word_timestamps=True,
    prepend_punctuations="\"'"¬ø([{-",
    append_punctuations="\"'.„ÄÇ,!?:)]}„ÄÅ"
)
```

### 9. Production Checklist

**Before Transcription:**
- [ ] Audio quality check (sample rate ‚â• 16kHz)
- [ ] Remove silence (VAD)
- [ ] Chunk long audio (smart chunking)
- [ ] Prepare domain-specific prompt

**During Transcription:**
- [ ] Use large-v3 model
- [ ] Set language explicitly ("th")
- [ ] Enable FP16 (GPU)
- [ ] Enable word timestamps
- [ ] Set temperature=0 (deterministic)

**After Transcription:**
- [ ] Clean hallucinations
- [ ] Fix punctuation
- [ ] Validate timestamps
- [ ] Generate SRT
- [ ] Quality check (sample review)

### 10. Platform-Specific Tips

**Google Colab:**
```python
# ‚úÖ Install Whisper
!pip install -q git+https://github.com/openai/whisper.git

# ‚úÖ Check GPU
!nvidia-smi

# ‚úÖ Mount Drive
from google.colab import drive
drive.mount('/content/drive')
```

**Kaggle:**
```python
# ‚úÖ Kaggle has GPU quota limits
# Use FP16 + batch processing to maximize efficiency
```

**Paperspace:**
```python
# ‚úÖ Persistent storage
# Save models to /storage/ not /tmp/
```

---

## Implementation ROI

**Current Situation (Without Skill):**
- Claude gives general Whisper advice
- You manually debug issues
- Trial & error: 2-3 hours per problem

**With Skill:**
- Claude instantly knows:
  - Thai optimization tricks
  - VAD integration
  - Hallucination fixes
  - Production best practices
- Time saved: 80% (30 min vs 2-3 hours)

**Worth it?** ‚úÖ **Absolutely!** Used in multiple active projects.

---
```

**‡∏Ç‡∏ô‡∏≤‡∏î:** ~1,500 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
**ROI:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô!)

---

### 2. **tts-synthesis-skill** (Text-to-Speech) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**‡∏ó‡∏≥‡πÑ‡∏°‡∏Ñ‡∏∏‡πâ‡∏°:**
- ‚úÖ ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô quantum-sync-v5 ‡πÅ‡∏•‡∏∞ video-translation-platform
- ‚úÖ Edge-TTS, AWS Polly, ElevenLabs ‡∏°‡∏µ quirks ‡πÄ‡∏¢‡∏≠‡∏∞
- ‚úÖ Timeline sync ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏û‡∏¥‡πÄ‡∏®‡∏©

```yaml
---
name: tts-synthesis-skill
description: Expert TTS (Text-to-Speech) synthesis for multi-platform voice generation. Use for: Edge-TTS, AWS Polly, ElevenLabs, Google Cloud TTS, voice cloning, SSML markup, prosody control, timeline-accurate synthesis, and production optimization.
---

# TTS Synthesis Expert Skill

## Platform Comparison

| Platform | Quality | Cost | Speed | Voice Cloning | Best For |
|----------|---------|------|-------|---------------|----------|
| **Edge-TTS** | ‚≠ê‚≠ê‚≠ê | Free | Fast | ‚ùå | Testing, prototypes |
| **AWS Polly** | ‚≠ê‚≠ê‚≠ê‚≠ê | $4/1M chars | Fast | ‚ùå | Production (standard) |
| **ElevenLabs** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | $1/1K chars | Medium | ‚úÖ | Production (premium) |
| **Google Cloud** | ‚≠ê‚≠ê‚≠ê‚≠ê | $4/1M chars | Fast | ‚ùå | Production (standard) |
| **Azure Neural** | ‚≠ê‚≠ê‚≠ê‚≠ê | $15/1M chars | Fast | ‚ùå | Enterprise |

## 1. Edge-TTS (Free!)

**Best Free Option:**

```python
import edge_tts
import asyncio

async def text_to_speech_edge(text, output_path, voice="en-US-AriaNeural"):
    """
    Edge-TTS synthesis (Microsoft voices)
    """
    communicate = edge_tts.Communicate(text, voice)
    await communicate.save(output_path)

# List available voices
async def list_voices():
    voices = await edge_tts.list_voices()
    for voice in voices:
        print(f"{voice['Name']}: {voice['Gender']} ({voice['Locale']})")

# Usage
asyncio.run(text_to_speech_edge(
    "Hello world",
    "output.mp3",
    voice="en-US-GuyNeural"  # Male voice
))
```

**Best Voices (English):**
- `en-US-AriaNeural` - Female, natural
- `en-US-GuyNeural` - Male, professional
- `en-US-JennyNeural` - Female, friendly
- `en-GB-SoniaNeural` - British female

**SSML Support:**

```python
ssml_text = """
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <prosody rate="0.9" pitch="-2st">
        This is slower and lower pitch.
    </prosody>
    <break time="500ms"/>
    <prosody rate="1.2">
        This is faster.
    </prosody>
</speak>
"""

await text_to_speech_edge(ssml_text, "output.mp3")
```

## 2. Timeline-Accurate Synthesis

**Problem:** SRT subtitles have exact timing, but TTS duration is unpredictable

**Solution: Timeline-Aware Merging**

```python
def calculate_required_duration(srt_segment):
    """
    Calculate exact duration from SRT timestamps
    """
    start_ms = timestamp_to_ms(srt_segment['start'])
    end_ms = timestamp_to_ms(srt_segment['end'])
    return end_ms - start_ms

def adjust_audio_duration(audio_path, target_duration_ms):
    """
    Stretch/compress audio to match target duration
    """
    from pydub import AudioSegment
    from pydub.effects import speedup

    audio = AudioSegment.from_file(audio_path)
    actual_duration = len(audio)  # ms

    # Calculate speed adjustment
    speed_ratio = actual_duration / target_duration_ms

    if abs(speed_ratio - 1.0) < 0.05:
        # Within 5% - acceptable
        return audio

    # Adjust speed (max ¬±20%)
    if speed_ratio > 1.2:
        speed_ratio = 1.2
    elif speed_ratio < 0.8:
        speed_ratio = 0.8

    adjusted = audio._spawn(
        audio.raw_data,
        overrides={
            "frame_rate": int(audio.frame_rate * speed_ratio)
        }
    ).set_frame_rate(audio.frame_rate)

    return adjusted
```

**Smart Silence Insertion:**

```python
def merge_with_timeline(segments, output_path):
    """
    Merge TTS audio with exact timeline from SRT
    """
    from pydub import AudioSegment

    final_audio = AudioSegment.silent(duration=0)
    current_time_ms = 0

    for segment in segments:
        target_start_ms = timestamp_to_ms(segment['start'])
        target_end_ms = timestamp_to_ms(segment['end'])
        target_duration_ms = target_end_ms - target_start_ms

        # Add silence gap if needed
        if target_start_ms > current_time_ms:
            silence_duration = target_start_ms - current_time_ms
            final_audio += AudioSegment.silent(duration=silence_duration)

        # Load and adjust segment audio
        audio = AudioSegment.from_file(segment['audio_path'])
        adjusted_audio = adjust_audio_duration(audio, target_duration_ms)

        # Append
        final_audio += adjusted_audio
        current_time_ms = target_end_ms

    # Export
    final_audio.export(output_path, format="mp3", bitrate="192k")
```

## 3. Voice Cloning (ElevenLabs)

**Upload Voice Sample:**

```python
from elevenlabs import clone, generate, play, set_api_key

set_api_key("your_api_key")

# Clone voice from samples
voice = clone(
    name="Custom Voice",
    description="Professional narrator",
    files=["sample1.mp3", "sample2.mp3", "sample3.mp3"]
)

# Generate with cloned voice
audio = generate(
    text="Hello, this is my cloned voice!",
    voice=voice,
    model="eleven_multilingual_v2"
)

# Save
with open("output.mp3", "wb") as f:
    f.write(audio)
```

## 4. Production Optimization

**Batch Processing (Save API Costs):**

```python
async def batch_synthesize(segments, voice, max_concurrent=5):
    """
    Process multiple segments concurrently
    """
    import asyncio
    from itertools import islice

    async def synthesize_one(segment):
        audio_path = f"segment_{segment['id']}.mp3"
        await text_to_speech_edge(
            segment['text'],
            audio_path,
            voice=voice
        )
        return {"id": segment['id'], "path": audio_path}

    # Process in batches of max_concurrent
    results = []
    for batch in batched(segments, max_concurrent):
        batch_results = await asyncio.gather(*[
            synthesize_one(seg) for seg in batch
        ])
        results.extend(batch_results)

    return results

def batched(iterable, n):
    """Batch iterator"""
    it = iter(iterable)
    while True:
        batch = list(islice(it, n))
        if not batch:
            break
        yield batch
```

## 5. Quality Optimization

**SSML for Better Prosody:**

```python
def enhance_with_ssml(text, segment_type="narration"):
    """
    Add SSML markup for natural speech
    """
    if segment_type == "narration":
        # Slower, deeper voice
        return f"""
        <speak>
            <prosody rate="0.9" pitch="-2st">
                {text}
            </prosody>
        </speak>
        """

    elif segment_type == "dialogue":
        # Natural conversational speed
        return f"""
        <speak>
            <prosody rate="1.0" pitch="0st">
                {text}
            </prosody>
        </speak>
        """

    elif segment_type == "emphasis":
        # Emphasize key points
        return f"""
        <speak>
            <emphasis level="strong">{text}</emphasis>
        </speak>
        """
```

**Audio Post-Processing:**

```python
from pydub import AudioSegment
from pydub.effects import normalize, compress_dynamic_range

def enhance_audio_quality(input_path, output_path):
    """
    Normalize and compress audio for consistent volume
    """
    audio = AudioSegment.from_file(input_path)

    # Normalize volume
    audio = normalize(audio)

    # Compress dynamic range
    audio = compress_dynamic_range(audio)

    # Export with high quality
    audio.export(
        output_path,
        format="mp3",
        bitrate="192k",
        parameters=["-q:a", "0"]  # Highest quality
    )
```

---

## ROI Analysis

**Current (Without Skill):**
- Trial & error with different TTS platforms
- Manual timeline sync (often fails)
- No optimization ‚Üí high costs

**With Skill:**
- ‚úÖ Know which platform for which use case
- ‚úÖ Timeline sync works first try
- ‚úÖ SSML optimization ‚Üí better quality
- ‚úÖ Batch processing ‚Üí 5x faster
- ‚úÖ Cost optimization

**Time Saved:** 2-3 hours per project
**Cost Saved:** 30-50% (batch processing + optimization)

**Worth it?** ‚úÖ **Absolutely!**
```

**‡∏Ç‡∏ô‡∏≤‡∏î:** ~1,200 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
**ROI:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

---

### 3. **ffmpeg-video-processing-skill** ‚≠ê‚≠ê‚≠ê‚≠ê

**‡∏ó‡∏≥‡πÑ‡∏°‡∏Ñ‡∏∏‡πâ‡∏°:**
- ‚úÖ ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô video dubbing workflow
- ‚úÖ FFmpeg ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å Claude ‡∏°‡∏±‡∏Å‡∏à‡∏≥‡∏ú‡∏¥‡∏î
- ‚úÖ ‡∏°‡∏µ gotchas ‡πÄ‡∏¢‡∏≠‡∏∞ (codec, sync issues)

```yaml
---
name: ffmpeg-video-processing-skill
description: Expert FFmpeg for video/audio processing. Use for: video/audio merging, codec conversion, subtitle burning, audio sync, stream mapping, quality optimization, and production-ready video processing.
---

# FFmpeg Expert Skill

## Common Tasks

### 1. Replace Audio in Video

```bash
# ‚úÖ CORRECT - Replace audio track
ffmpeg -i input_video.mp4 -i new_audio.mp3 \
  -c:v copy \  # Copy video (no re-encode)
  -c:a aac \   # Encode audio to AAC
  -b:a 192k \  # Audio bitrate
  -map 0:v:0 \ # Video from first input
  -map 1:a:0 \ # Audio from second input
  -y output.mp4

# ‚ùå WRONG - Re-encodes video (slow + quality loss)
ffmpeg -i input_video.mp4 -i new_audio.mp3 -y output.mp4
```

### 2. Audio/Video Sync

```bash
# Delay audio by 500ms
ffmpeg -i video.mp4 -i audio.mp3 \
  -itsoffset 0.5 -i audio.mp3 \
  -map 0:v -map 1:a \
  -c:v copy -c:a aac \
  output.mp4

# Advance audio by 200ms (negative offset)
ffmpeg -i video.mp4 -itsoffset -0.2 -i audio.mp3 \
  -map 0:v -map 1:a \
  -c:v copy -c:a aac \
  output.mp4
```

### 3. Burn Subtitles

```bash
# Burn SRT into video
ffmpeg -i video.mp4 -vf subtitles=subtitle.srt \
  -c:a copy \
  output.mp4

# With custom subtitle style
ffmpeg -i video.mp4 \
  -vf "subtitles=subtitle.srt:force_style='FontName=Arial,FontSize=24,PrimaryColour=&H00FFFF'" \
  -c:a copy \
  output.mp4
```

### 4. Extract Audio

```bash
# Extract audio as MP3
ffmpeg -i video.mp4 -vn -acodec libmp3lame -q:a 2 audio.mp3

# Extract audio as WAV (lossless)
ffmpeg -i video.mp4 -vn -acodec pcm_s16le audio.wav
```

### 5. Quality Optimization

```bash
# High quality H.264 encode
ffmpeg -i input.mp4 \
  -c:v libx264 \
  -preset slow \     # slow = better compression
  -crf 18 \          # 18 = near-lossless (0-51, lower = better)
  -c:a aac \
  -b:a 192k \
  output.mp4
```

---

**Worth it?** ‚≠ê‚≠ê‚≠ê‚≠ê (‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô video projects)
```

**‡∏Ç‡∏ô‡∏≤‡∏î:** ~800 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
**ROI:** ‚≠ê‚≠ê‚≠ê‚≠ê

---

## ‚≠ê Priority 2: Translation & Localization Skills

### 4. **thai-english-translation-skill** ‚≠ê‚≠ê‚≠ê‚≠ê

**‡∏ó‡∏≥‡πÑ‡∏°‡∏Ñ‡∏∏‡πâ‡∏°:**
- ‚úÖ ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô video-translater project
- ‚úÖ Thai ‚Üî English ‡∏°‡∏µ nuances ‡πÄ‡∏¢‡∏≠‡∏∞
- ‚úÖ Forex/technical terms ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

```yaml
---
name: thai-english-translation-skill
description: Expert Thai ‚Üî English translation for technical content. Use for: Forex/trading terminology, Thai idioms, context-aware translation, cultural nuances, formal vs informal Thai, and subtitle translation optimization.
---

# Thai-English Translation Expert

## Domain-Specific Dictionaries

### Forex/Trading Terms (50+)
```python
FOREX_TERMS = {
    "‡πÅ‡∏ô‡∏ß‡∏ï‡πâ‡∏≤‡∏ô": "resistance level",
    "‡πÅ‡∏ô‡∏ß‡∏£‡∏±‡∏ö": "support level",
    "‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πå": "trend",
    "‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô": "candlestick chart",
    "‡πÄ‡∏ö‡∏£‡∏Å‡πÄ‡∏≠‡∏≤‡∏ó‡πå": "breakout",
    ...
}
```

### Thai Idioms (105+)
```python
THAI_IDIOMS = {
    "‡πÄ‡∏≠‡∏≤‡πÄ‡∏Ç‡πá‡∏°‡∏õ‡∏±‡πà‡∏ô‡∏´‡∏≤‡∏à‡∏°‡∏π‡∏Å‡∏ä‡πâ‡∏≤‡∏á": "looking for a needle in a haystack",
    "‡∏Ç‡∏µ‡πà‡∏ä‡πâ‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏ï‡∏±‡πä‡∏Å‡πÅ‡∏ï‡∏ô": "using a sledgehammer to crack a nut",
    ...
}
```

---

**Worth it?** ‚≠ê‚≠ê‚≠ê‚≠ê
```

**‡∏Ç‡∏ô‡∏≤‡∏î:** ~600 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
**ROI:** ‚≠ê‚≠ê‚≠ê‚≠ê

---

## ‚≠ê Priority 3: Automation & Workflow Skills

### 5. **video-localization-workflow-skill** ‚≠ê‚≠ê‚≠ê

**‡∏ó‡∏≥‡πÑ‡∏°‡∏Ñ‡∏∏‡πâ‡∏°:**
- ‚úÖ ‡∏£‡∏ß‡∏° workflow ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (Whisper ‚Üí Translation ‚Üí TTS ‚Üí FFmpeg)
- ‚úÖ ‡∏°‡∏µ checkpoints, error handling
- ‚úÖ Production best practices

```yaml
---
name: video-localization-workflow-skill
description: End-to-end video localization workflow combining transcription, translation, and voice synthesis. Use for: Thai ‚Üí English video dubbing, checkpoint systems, error recovery, quality control, and production pipeline orchestration.
---
```

**‡∏Ç‡∏ô‡∏≤‡∏î:** ~900 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
**ROI:** ‚≠ê‚≠ê‚≠ê

---

## ‚ùå Skills ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á

### 1. **Machine Learning General** (‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°)
- ‚ùå Claude ‡∏£‡∏π‡πâ ML ‡∏î‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
- ‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ company-specific ‡∏ó‡∏µ‡πà‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏û‡∏≠

### 2. **Python Best Practices** (‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°)
- ‚ùå Claude expert Python ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß

### 3. **TikTok Automation** (‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°)
- ‚ö†Ô∏è Project archived
- ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

---

## üìä ‡∏™‡∏£‡∏∏‡∏õ Skills ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥

| Skill | Priority | Lines | ROI | ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Projects |
|-------|----------|-------|-----|-----------------|
| **whisper-transcription-skill** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ~1,500 | ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å | video-translater, video-translation-platform |
| **tts-synthesis-skill** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ~1,200 | ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å | quantum-sync-v5, video-translation-platform |
| **ffmpeg-video-processing-skill** | ‚≠ê‚≠ê‚≠ê‚≠ê | ~800 | ‡∏™‡∏π‡∏á | ‡∏ó‡∏∏‡∏Å video projects |
| **thai-english-translation-skill** | ‚≠ê‚≠ê‚≠ê‚≠ê | ~600 | ‡∏™‡∏π‡∏á | video-translater |
| **video-localization-workflow-skill** | ‚≠ê‚≠ê‚≠ê | ~900 | ‡∏Å‡∏•‡∏≤‡∏á-‡∏™‡∏π‡∏á | video-translation-platform |

**‡∏£‡∏ß‡∏°:** ~5,000 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î (6% ‡∏Ç‡∏≠‡∏á marketing skills ‡∏ó‡∏µ‡πà‡∏°‡∏µ)
**‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á:** 2-3 ‡∏ß‡∏±‡∏ô
**ROI:** ‡∏Ñ‡∏∏‡πâ‡∏°‡∏°‡∏≤‡∏Å (‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡πÉ‡∏ô active projects)

---

## üéØ ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥

### Phase 1: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å Priority ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
1. ‚úÖ **whisper-transcription-skill** (‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
2. ‚úÖ **tts-synthesis-skill** (‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤)
3. ‚úÖ **ffmpeg-video-processing-skill** (‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö video merging)

### Phase 2: Translation & Workflow
4. ‚úÖ **thai-english-translation-skill**
5. ‚úÖ **video-localization-workflow-skill**

### Phase 3: Optional (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏ß‡∏•‡∏≤)
- SRT manipulation skill
- Audio enhancement skill
- Video quality optimization skill

---

## üí° ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Marketing Skills

### Marketing Skills (39 skills, 82,873 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)
- ‚úÖ ROI: ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (conversion +5-10x)
- ‚úÖ ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô copy/marketing
- ‚úÖ Claude base knowledge: 1-2%
- ‚úÖ Improvement: +900%

### Audio/Video Skills (5 skills, ~5,000 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)
- ‚úÖ ROI: ‡∏™‡∏π‡∏á (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô active projects)
- ‚úÖ ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥ video localization
- ‚ö†Ô∏è Claude base knowledge: 40-50%
- ‚ö†Ô∏è Improvement: +100-200%

**‡∏™‡∏£‡∏∏‡∏õ:**
- Marketing skills = ‡∏Ñ‡∏∏‡πâ‡∏°‡∏Å‡∏ß‡πà‡∏≤ (base knowledge ‡∏ô‡πâ‡∏≠‡∏¢)
- Audio/Video skills = ‡∏Ñ‡∏∏‡πâ‡∏°‡∏î‡∏µ (‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏ö‡πà‡∏≠‡∏¢)
- ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏Ñ‡∏∏‡πâ‡∏° ‡πÅ‡∏ï‡πà marketing ‡∏Ñ‡∏∏‡πâ‡∏°‡∏Å‡∏ß‡πà‡∏≤

---

## ‚úÖ ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢

**‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ (‡∏°‡∏µ 39 Marketing Skills ‡πÅ‡∏•‡πâ‡∏ß):**
1. ‚úÖ ‡πÉ‡∏ä‡πâ Marketing skills ‡∏ï‡πà‡∏≠ (‡∏Ñ‡∏∏‡πâ‡∏°‡∏°‡∏≤‡∏Å!)
2. üëÄ ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï audio/video pain points
3. üìä ‡∏ß‡∏±‡∏î ROI ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á

**‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï (‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ö‡πà‡∏≠‡∏¢):**
1. ‡∏™‡∏£‡πâ‡∏≤‡∏á **whisper-transcription-skill** ‡∏Å‡πà‡∏≠‡∏ô (‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
2. ‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ **tts-synthesis-skill**
3. ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô **ffmpeg-video-processing-skill**

**Rule of Thumb:**
> "‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ã‡πâ‡∏≥ >5 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á/‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå"
>
> "‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏ö‡πà‡∏≠‡∏¢ ‚Üí ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏µ‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á"

---

**‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó:** 2025-10-24
**‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞:** ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à - ‡∏£‡∏≠ user feedback ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á
